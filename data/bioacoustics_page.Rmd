# **CalCOFI Bioacoustics**

Bioacoustics refers to the study of the sounds produced by marine mammals and their underwater acoustic environment. Bioacoustic studies of marine mammals involve the collection, analysis, and interpretation of these sounds to gain insights into various aspects of their behavior, ecology, and conservation.


<br/>
<center>
  <img src="CC0808SB02_0815213000_second_60_to_120.png" height="300" width="750">
  
###### *Spectrogram with all identifiable calls present (blue whale A, B, and D calls / fin whale 20 hz and 40 hz calls)*

</center>
<br/>


## **How is Bioacoustic Data Collected and Visualized?**

CalCOFI ships deploy sonobuoys at varying depths to record marine mammal acoustics for hours at a time. These devices typically pick up 2000 samples of sound waves per second, which are saved as wav files for further manipulation. A series of Fourier transforms are then applied to short, overlapping segments of the recorded signals to convert the data from the time domain (amplitude varying with time) to the frequency domain. Additional features of the sound wave can then be extracted such as a signal's frequency spectrum and magnitude. Using this acquired information, a spectrogram is constructed by piecing together the audio segments into one continuous plot that provides a time-varying representation of a signal's frequency content. Duration is on the x-axis, frequency is on the y-axis, and magnitude is represented by color. The end product is a colorful visual capture of recorded sound that contains anything from whale songs to white noise from ships, as you can see in the figure above.

<br/><center>
#### **Listen to actual recordings by pressing the play buttons!**
</center>


<center>
<div style="display: flex; justify-content: center;">
<div style="display: flex;">
  <figure style="margin-right: 20px;">
    <img src="blue_whale_B_call.png" height="200" width="450">
    <figcaption><em>Blue Whale B Call</em><figcaption>
  </figure>
  <figure>
    <img src="fin_whale_pulse.png" height="200" width="450">
    <figcaption><em>Fin Whale Pulse</em><figcaption>
  </figure>
</div>
</center>


<center>
<div style="display: flex;">
  <audio controls style="margin-left: 160px;">
    <source src="ringtoneBlue.mp3" type="audio/mp3">
  </audio>
  <audio controls style="margin-left: 180px;">
    <source src="ringtoneFin.mp3" type="audio/mp3">
  </audio>
</div>
</center>

<br/><br/>

## **Faster r-CNN Deep Learning Model**

A faster r-CNN ResNet-50 model was built to automate the identification of whale calls and classify them given spectrograms transformed from wav files of collected bioacoustic data as input. Although functional, imperfect accuracy was likely due to the abundance of noise and insignificant signals scattered throughout the data as shown below. These factors made it difficult for the model to distinguish between the actual calls and undesired visuals.

<center>
  <img src="random noisy spectrogram" height="300" width="750">
  
###### *Spectrogram with a bunch of noise*

</center>

The solution was to produce a preprocessing pipeline that would eliminate this noise, and consequently, increase both model runtime efficiency and classification accuracy. The following is a small scale demonstration of the final pipeline with code and output examples:


### **Step 1: Resizing the Data**

```{python eval=TRUE}
def draw_img(ax, img_vector, h=141, w=601):
    """
    1. takes img_vector,
    2. reshapes into right dimensions,
    3. draws the resulting image
    """
    ax.imshow( (img_vector).reshape(h,w), cmap=plt.cm.gray)
    plt.xticks(())
    plt.yticks(())
```

### **Step 2: Vectorizing/Stacking the Images**

```{python eval=TRUE}
for index, row in unique_annotation.iterrows():
    image = Image.open(row['spectrogram_path'])
    pixel_values = np.array(list(image.getdata()))
    data_matrix.append(pixel_values)
    
stacked_specs = np.vstack(data_matrix)
```

### **Step 3: Singular Value Decomposition**

```{python eval=TRUE}
U, S, T = np.linalg.svd(original_data, full_matrices=False)
US = U*S

svd_data = US @ T
svd_data_scaled = scaler.inverse_transform(svd_data)
```

### **Step 4: Median Filtering and Background Subtraction**

```{python eval=TRUE}
for i in range(len(T)):
    feature = np.copy(T[i].reshape((141, 601)))
    feature = median_filter(feature, size = 3)

    for j in range(feature.shape[1]):
        column = feature[:, j]
        percentile_value = np.percentile(column, 10)
        feature[:, j] = column - percentile_value
        feature[:, j][feature[:, j] < 0] = 0
        
    signal_enhanced_features[i] = feature.flatten()
```

### **Step 5: Reconstruction**

```{python eval=TRUE}
matrix = US[:, 0:150] @ signal_enhanced_features[0:150, :]
matrix = US @ signal_enhanced_features
matrix_scaled = scaler.inverse_transform(matrix)
matrix_scaled = np.where(matrix_scaled < 0, 0, matrix_scaled)
```

### **Step 6: Background Subtraction on Reconstruction**

```{python eval=TRUE}
matr_sub = np.zeros_like(matrix_scaled)

for i in range(len(matrix_scaled)):
    spec = np.copy(matrix_scaled[i].reshape((141, 601)))

    for j in range(spec.shape[1]):
        column = spec[:, j]
        percentile_value = np.percentile(column, 60)
        spec[:, j] = column - percentile_value
        spec[:, j][spec[:, j] < 0] = 0

    matr_sub[i] = spec.flatten()
```




